// This module is included in the following assembly:
//
// * cicd/pipelines/using-tekton-results-for-openshift-pipelines-observability.adoc

:_content-type: PROCEDURE
[id="results-storage_{context}"]
= Preparing storage for logging information

{tekton-results} uses separate storage for logging information related to pipeline runs and task runs. You can configure any one of the following types of storage:

* Persistent volume claim (PVC) on your {product-title} cluster
* Google Cloud Storage
* S3 bucket storage

.Procedure

Complete one of the following procedures:

* To use a PVC, complete the following steps:
.. Create a file named `pvc.yaml` with the following definition for the PVC:
+
[source,yaml]
----
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: tekton-logs
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 1Gi
----
.. Apply the definition by entering the following command:
+
[source,terminal]
----
$ oc apply -n openshift-pipelines -f pvc.yaml
----

* To use Google Cloud Storage, complete the following steps:
.. Create an application credentials file by using the `gcloud` command. For instructions about providing application credentials in a file, see link:https://cloud.google.com/docs/authentication/application-default-credentials#personal[User credentials provided by using the gcloud CLI] in the Google Cloud documentation.
.. Create a secret from the application credentials file by entering the following command:
+
[source,terminal]
----
$ oc create secret generic gcs-credentials \
  --from-file=$HOME/.config/gcloud/application_default_credentials.json \
  -n openshift-pipelines
----
+
Adjust the path and filename of the application credentials file as necessary.

* To use S3 bucket storage, complete the following steps:
.. Create a file named `s3_secret.yaml` with the following content:
+
[source,yaml]
----
  apiVersion: v1
  kind: Secret
  metadata:
    name: my_custom_secret
    namespace: tekton-pipelines
  type: Opaque
  stringData:
    S3_BUCKET_NAME: bucket1 # <1>
    S3_ENDPOINT: https://example.localhost.com # <2>
    S3_HOSTNAME_IMMUTABLE: "false"
    S3_REGION: region-1 # <3>
    S3_ACCESS_KEY_ID: "1234" # <4>
    S3_SECRET_ACCESS_KEY: secret_key # <5>
    S3_MULTI_PART_SIZE: "5242880"
----
<1> The name of the S3 storage bucket
<2> The S3 API endpoint URL
<3> The S3 region
<4> The S3 access key ID
<5> The S3 secret access key

.. Create a secret from the file by entering the following command:
+
[source,terminal]
----
$ oc create secret generic s3-credentials \
  --from-file=s3_secret.yaml -n openshift-pipelines
----
