// This module is included in the following assembly:
//
// * cicd/pipelines/using-tekton-results-for-openshift-pipelines-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="enabling-results_{context}"]
= Enabling {tekton-results}

[role="_abstract"]
The {pipelines-shortname} Operator installs the Results component by default when installint {pipelines-shortname}. To enable {tekton-results}, you must provide the required resources and then make changes in the `TektonConfig` custom resource (CR).

.Prerequisites

* You installed {pipelines-shortname} using the Operator.
* You prepared a secret with the SSL certificate.
* You prepared storage or LokiStack forwarding for the logging information.
* You prepared a secret with the database credentials.

.Procedure

. Edit the `TektonResults` CR to make the following changes in the `result` spec.
+
[source,yaml]
----
apiVersion: operator.tekton.dev/v1alpha1
apiVersion: operator.tekton.dev/v1alpha1
kind: TektonConfig
metadata:
  name: config
spec:
  result:
    disabled: false
    targetNamespace: openshift-pipelines
    logs_api: true
    log_level: debug
    db_port: 5432
    db_host: tekton-results-postgres-service.openshift-pipelines.svc.cluster.local
    logs_path: /logs
    logs_type: File
    logs_buffer_size: 32768
    auth_disable: false
    db_enable_auto_migration: true
    server_port: 8080
    prometheus_port: 9090
----

. Add storage or forwarding configuration for logging information to the same specification in the `TektonConfig` CR:
** If you configured a persistent volume claim (PVC), add the following line to provide the name of the PVC:
+
[source,yaml]
----
    logging_pvc_name: tekton-logs
----
** If you configured Google Cloud Storage, add the following lines to provide the secret name, the credentials file name, and the name of the Google Cloud Storage bucket:
+
[source,yaml]
----
    gcs_creds_secret_name: gcs-credentials
    gcs_creds_secret_key: application_default_credentials.json # <1>
    gcs_bucket_name: bucket-name #<2>
----
<1> Provide the name, without the path, of the application credentials file that you used when creating the secret.
<2> Provide the name of a bucket in Google Cloud Storage. {tekton-chains} uses this bucket to store logging information for pipeline runs and task runs.
** If you configured S3 bucket storage, add the following line to provide the name of the S3 secret:
+
[source,yaml]
----
    secret_name: s3-credentials
----
** If you configured LokiStack forwarding, add the following lines to enable forwarding logging information to LokiStack:
+
[source,yaml]
----
    loki_stack_name: logging-loki # <1>
    loki_stack_namespace: openshift-logging # <2>
----
<1> The name of the `LokiStack` CR, typically `logging-loki`.
<2> The name of the namespace where LokiStack is deployed, typically `openshift-logging`.

. Optional: If you want to use an external PostgreSQL database server to store {tekton-results} information, add the following lines to the file, instead of the values for `db_host` and `db_port` taht are listed in the previous step:
+
[source,yaml]
----
    db_host: postgres.internal.example.com # <1>
    db_port: 5432 # <2>
    is_external_db: true
----
<1> The host name for the PostgreSQL server.
<2> The port for the PostgreSQL server.

. Save the `TektonConfig` CR.

. Expose the route for the {tekton-results} service API by entering the following command:
+
[source,terminal]
----
$ oc create route -n openshift-pipelines \ 
  passthrough tekton-results-api-service \
  --service=tekton-results-api-service --port=8080
----
