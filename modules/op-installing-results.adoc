// This module is included in the following assembly:
//
// * cicd/pipelines/using-tekton-results-for-openshift-pipelines-observability.adoc

:_content-type: PROCEDURE
[id="installing-results_{context}"]
= Installing {tekton-results}

[role="_abstract"]
To install {tekton-results}, you must provide the required resources and then create and apply a `TektonResult` custom resource (CR). The {pipelines-shortname} Operator installs the Results services when you apply the `TektonResult` custom resource.

.Prerequisites

* You installed {pipelines-shortname} using the Operator.
* You prepared a secret with the SSL certificate.
* You prepared storage for the logging information.
* You prepared a secret with the database credentials.

.Procedure

. Create the resource definition file named `result.yaml` based on the following example. You can adjust the settings as necessary.
+
[source,yaml]
----
  apiVersion: operator.tekton.dev/v1alpha1
  kind: TektonResult
  metadata:
    name: result
  spec:
    targetNamespace: openshift-pipelines
    logs_api: true
    log_level: debug
    db_port: 5432
    db_user: result
    db_host: tekton-results-postgres-service.openshift-pipelines.svc.cluster.local
    logs_path: /logs
    logs_type: File
    logs_buffer_size: 32768
    auth_disable: true
    tls_hostname_override: tekton-results-api-service.openshift-pipelines.svc.cluster.local
    db_enable_auto_migration: true
    server_port: 8080
    prometheus_port: 9090
----

. Add configuration for the storage for logging information to this file:
** If you configured a persistent volume claim (PVC), add the following line to provide the name of the PVC:
+
[source,yaml]
----
    logging_pvc_name: tekton-logs
----
** If you configured Google Cloud Storage, add the following lines to provide the secret name, the credentials file name, and the name of the Google Cloud Storage bucket:
+
[source,yaml]
----
    gcs_creds_secret_name: gcs-credentials
    gcs_creds_secret_key: application_default_credentials.json # <1>
    gcs_bucket_name: bucket-name #<2>
----
<1> Provide the name, without the path, of the application credentials file that you used when creating the secret.
<2> Provide the name of a bucket in Google Cloud Storage. {tekton-chains} uses this bucket to store logging information for pipeline runs and task runs.
** If you configured S3 bucket storage, add the following line to provide the name of the S3 secret:
+
[source,yaml]
----
    secret_name: s3-credentials
----

. Optional: If you want to use an external PostgreSQL database server to store {tekton-results} information, add the following lines to the file:
+
[source,yaml]
----
    db_host: postgres.internal.example.com # <1>
    db_port: 5432 # <2>
    db_user: user # <3>
    is_external_db: true
----
<1> The host name for the PostgreSQL server.
<2> The port for the PostgreSQL server.
<3> The PostgreSQL user name.

. Apply the resource definition by entering the following command:
+
[source,terminal]
----
$ oc apply -n openshift-pipelines -f result.yaml
----

. Expose the route for the {tekton-results} service API by entering the following command:
+
[source,terminal]
----
$ oc create route -n openshift-pipelines \
  passthrough tekton-results-api-service \
  --service=tekton-results-api-service --port=8080
----
